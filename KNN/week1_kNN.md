# MachineLearning


## kNN Algorithm
__kNN Algorithm__ <https://www.elastic.co/kr/what-is/knn> 참고함! 


### KNN (K 최근접 이웃 알고리즘)

: 새로운 데이터가 들어왔을 때 특정 값으로 분류하는데 현재 데이터와 가장 가까운 k개의 데이터를  찾아 가장 많은 분류값으로 새로운 데이터를 분류하는 알고리즘. 지도학습에 속함.

- 지도학습 분류와 회귀?
    - 분류 : 샘플을 몇 개의 클래스 중 하나로 분류
    - 회귀 : 임의의 어떤 숫자를 예측. 두 변수 사이의 상관관계를 분석하는 방법
        - 예) 경제 성장률 예측, 배달이 도착할 시간, 생선의 무게 예측



### 1) k-최근접 이웃 분류 알고리즘

- 예측하려는 샘플에 가장 가까운 샘플 k개를 선택
- 이 샘플들의 클래스를 확인하여 다수의 클래스를 새로운 샘플의 클래스로 예측
- 즉, 최근접 이웃들의 최빈값

→ 끼리끼리 모이는 느낌. 유사한 요소가 서로 가까이에  위치.



### 2) k-최근접 이웃 회귀 알고리즘

- 예측하려는 샘플에 가장 가까운 샘플 k개를 선택
- 이웃한 샘플의 타깃은 클래스가 아닌 수치
- 즉, 가장 가까운 이웃들의 평균



### kNN 거리 메트릭을 계산하는 4가지 유형

kNN 알고리즘의 핵심은 새로운 데이터 지점과 다른 데이터 지점 사이의 거리를 결정하는 것. 거리 메트릭을 결정하면 결정 경계가 가능해져 다양한 데이터 지점 영역을 만듬.


- 유클리드 거리 : 가장 일반적. 새로운 데이터 지점과 다른 측정 대상 지점 사이의 직선거리 측정
- 맨해튼 거리 : 두 지점 사이의 절댓값 측정. 그리드 상에 표시하여 어떻게 이동하는지
- 민코프스키 거리 : 유클리드와 맨해튼을 일반화. 정규화된 벡터 공간에서 사용
- 해밍 거리: 중첩 메트릭. 길이가 같은 두 스트링 사이의 거리를 측정. 오류 탐지 및 오류 수정 코드에 유용





### 최적의 k값 선택하는 방법

최적의 k 값을 선택하려면 몇 가지 값을 실험하여 가장 적은 수의 오류로 가장 정확한 예측을 생성하는 k 값을 찾아야함.

- k 값이 낮으면 예측이 불안정해짐.
- k값이 높으면 잡음이 끼게됨.

→ 높은 분산과 높은 편향 사이의 균형을 이루는 k 값을 찾아야함.




### kNN 알고리즘의 장점과 한계

- 장점 : 간단함, 적응력, 쉽게 프로그래밍 가능
- 한계 : 확장하기 어려움, 차원의 저주, 과적합


  


### 주요 kNN 사용 사례

- 관련성 순위, 이미지 또는 동영상에 대한 유사성 검색 → 분류 분석에 사용될 때 다양한 용도로 사용
